{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dependencies and Setup\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import time \n",
    "import datetime \n",
    "import numpy as np\n",
    "from config1 import username1,password1,host1,port1,database1\n",
    "from config2 import username2,password2,host2,port2,database2\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, func, inspect, desc\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import requests\n",
    "import json\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define Urls for the Johns Hopkins Data\n",
    "confirm_url='https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "death_url='https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "\n",
    "# Read in COVID-19 Files\n",
    "confirm_df=pd.read_csv(confirm_url, error_bad_lines=False)\n",
    "death_df=pd.read_csv(death_url, error_bad_lines=False)\n",
    "# Rename column to match orders data\n",
    "confirm_df=confirm_df.rename(columns={'Province_State': 'State'})\n",
    "death_df=death_df.rename(columns={'Province_State': 'State'})\n",
    "\n",
    "# Clean Confirm data Rows - Remove data for territories and cruise ships, format FIPS\n",
    "confirm_df=confirm_df[confirm_df.Admin2 != 'Unassigned']\n",
    "confirm_df=confirm_df.dropna()\n",
    "confirm_df=confirm_df[~confirm_df['Admin2'].astype(str).str.startswith('Out of')]\n",
    "confirm_df=confirm_df[confirm_df.Admin2 != 'Out of*']\n",
    "confirm_df=confirm_df.reset_index()\n",
    "confirm_df['FIPS']=confirm_df.FIPS.map('{0:0>5.0f}'.format)\n",
    "\n",
    "# Clean Death Data Rows - Remove data for territories and cruise ships\n",
    "death_df=death_df[death_df.Admin2 != 'Unassigned']\n",
    "death_df=death_df.dropna()\n",
    "death_df=death_df[~death_df['Admin2'].astype(str).str.startswith('Out of')]\n",
    "death_df=death_df[death_df.Admin2 != 'Out of*']\n",
    "death_df=death_df.reset_index()\n",
    "death_df['FIPS']=death_df.FIPS.map('{0:0>5.0f}'.format)\n",
    "\n",
    "# Pull County data and drop unessacery columns\n",
    "county_population=death_df[['FIPS','Population']]\n",
    "county_population=county_population.rename(columns={'FIPS':'fips','Population':'population'})\n",
    "county_cases=confirm_df.drop(columns=['UID','index','Country_Region','Lat','Long_','iso2','iso3','code3','Combined_Key'])\n",
    "county_deaths=death_df.drop(columns=['UID','index','Country_Region','Lat','Long_','iso2','iso3','code3','Combined_Key','Population'])\n",
    "\n",
    "# rename columns for uniformity\n",
    "county_cases=county_cases.rename(columns={'Admin2':'County'})\n",
    "county_deaths=county_deaths.rename(columns={'Admin2':'County'})\n",
    "\n",
    "# Pull the most up to date date to be used later\n",
    "last_date=county_cases.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find daily totals - county cases\n",
    "county_cases_daily=county_cases.copy()\n",
    "[r,c]=county_cases_daily.shape\n",
    "# Loop through every row then column\n",
    "for j in range (0,r):\n",
    "    last=0\n",
    "    for i in range (3,c):\n",
    "        # Subtract totals to leave you with the new cases from the day before\n",
    "        current=county_cases_daily.iloc[j,i]-last\n",
    "        last=county_cases_daily.iloc[j,i]\n",
    "        county_cases_daily.iat[j,i]=current\n",
    "        \n",
    "\n",
    "#Find daily totals - county deaths\n",
    "county_deaths_daily=county_deaths.copy()\n",
    "[r,c]=county_deaths_daily.shape\n",
    "# Loop through every row then column\n",
    "for j in range (0,r):\n",
    "    last=0\n",
    "    for i in range (3,c):\n",
    "        # Subtract totals to leave you with the new cases from the day before\n",
    "        current=county_deaths_daily.iloc[j,i]-last\n",
    "        last=county_deaths_daily.iloc[j,i]\n",
    "        county_deaths_daily.iat[j,i]=current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine State Data\n",
    "state_confirms=confirm_df.groupby('State').sum()\n",
    "# Format state data\n",
    "state_confirms=state_confirms.drop(columns=['UID','code3','index','Lat','Long_'])\n",
    "state_deaths=death_df.groupby('State').sum()\n",
    "# Format state data\n",
    "state_deaths=state_deaths.reset_index()\n",
    "state_cases=state_confirms.reset_index()\n",
    "# Isolate population data\n",
    "state_population=state_deaths[['State','Population']]\n",
    "# Drop unessacery columns\n",
    "state_deaths=state_deaths.drop(columns=['UID','code3','index','Lat','Long_','Population'])\n",
    "\n",
    "\n",
    "#Find daily totals - state cases\n",
    "state_cases_daily=state_cases.copy()\n",
    "[r,c]=state_cases_daily.shape\n",
    "# Loop through every row then column\n",
    "for j in range (0,r):\n",
    "    last=0\n",
    "    for i in range (2,c):\n",
    "        # Subtract totals to leave you with the new cases from the day before\n",
    "        current=state_cases_daily.iloc[j,i]-last\n",
    "        last=state_cases_daily.iloc[j,i]\n",
    "        state_cases_daily.iat[j,i]=current\n",
    "\n",
    "#Find daily totals - state deaths\n",
    "state_deaths_daily=state_deaths.copy()\n",
    "[r,c]=state_deaths_daily.shape\n",
    "# Loop through every row then column\n",
    "for j in range (0,r):\n",
    "    last=0\n",
    "    for i in range (2,c):\n",
    "        # Subtract totals to leave you with the new cases from the day before\n",
    "        current=state_deaths_daily.iloc[j,i]-last\n",
    "        last=state_deaths_daily.iloc[j,i]\n",
    "        state_deaths_daily.iat[j,i]=current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify url for the order dates\n",
    "url = 'https://www.finra.org/rules-guidance/key-topics/covid-19/shelter-in-place'\n",
    "\n",
    "# Parse HTML Object \n",
    "response = requests.get(url)\n",
    "soup = bs(response.text, 'lxml')\n",
    "\n",
    "# Read Tables\n",
    "tables = pd.read_html(url)\n",
    "table = tables[0]\n",
    "\n",
    "# Remove Excess Columns\n",
    "orders = table[['State', 'Order Date', 'Order Expiration Date']]\n",
    "\n",
    "# Remove Special Charectors from State names\n",
    "orders['State'] = [re.sub(r'[^\\w]', ' ', state) for state in orders['State']]\n",
    "\n",
    "# Initialize lists for date formating\n",
    "od=orders['Order Date']\n",
    "dates=[]\n",
    "\n",
    "# Loop through end dates\n",
    "for date in od:\n",
    "    # Split words and select formatted dates\n",
    "    split = date.split()\n",
    "    res = [i for i in split if '/' in i]\n",
    "    try:\n",
    "        # Select the date and reformate into standard form\n",
    "        res=res[0]\n",
    "        splitdate = res.split('/')\n",
    "        # If the year is formated with a 2 digit year, assume that we are in the 2000s mellinia\n",
    "        if len(splitdate[2]) == 2:\n",
    "            splitdate[2]='20'+splitdate[2]\n",
    "        formated = datetime.date(int(splitdate[2]),int(splitdate[0]),int(splitdate[1]))\n",
    "        datestr=str(formated)\n",
    "    except:\n",
    "        # Add null sets for states without expiration dates\n",
    "        datestr=np.nan\n",
    "    # Add formatted dates to list\n",
    "    dates.append(datestr)\n",
    "# Add dates back in\n",
    "orders['Order Date']=dates\n",
    "    \n",
    "# Initialize lists for date formating\n",
    "oed= orders['Order Expiration Date']\n",
    "dates=[]\n",
    "\n",
    "# Loop through end dates\n",
    "for date in oed:\n",
    "    # Split words and select formatted dates\n",
    "    split = date.split()\n",
    "    res = [i for i in split if '/' in i]\n",
    "    try:\n",
    "        # Select the date and reformate into standard form\n",
    "        res=res[0]\n",
    "        splitdate = res.split('/')\n",
    "        # If the year is formated with a 2 digit year, assume that we are in the 2000s mellinia\n",
    "        if len(splitdate[2]) == 2:\n",
    "            splitdate[2]='20'+splitdate[2]\n",
    "        formated = datetime.date(int(splitdate[2]),int(splitdate[0]),int(splitdate[1]))\n",
    "        datestr=str(formated)\n",
    "    except:\n",
    "        # Add null sets for states without expiration dates\n",
    "        datestr=np.nan\n",
    "    # Add formatted dates to list\n",
    "    dates.append(datestr)\n",
    "# Add dates back in and correct index\n",
    "orders['Order Expiration Date']=dates\n",
    "\n",
    "# Format to merge and Merge\n",
    "orders['State']=[x.strip() for x in orders['State']]\n",
    "orders=orders.merge(state_population,on='State',how='right')\n",
    "# Rename columns to be compatible with sql\n",
    "orders=orders.rename(columns={'State':'state','Order Date': 'order_date','Order Expiration Date':'order_expiration_date','Population':'population'})\n",
    "# Sead state_heatmap matrix with state names and pop\n",
    "heatmap=orders.copy()\n",
    "# Drop unessacery columns\n",
    "orders=orders.drop(columns=['population'])\n",
    "orders=orders.sort_values(by=['state'])\n",
    "# Reset index for sql to read correctly\n",
    "orders=orders.reset_index()\n",
    "orders=orders.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for heatmap\n",
    "# Pull state names and current totals\n",
    "state_names=state_cases.iloc[:,0]\n",
    "state_totals=state_cases.iloc[:,-1]\n",
    "state_total_deaths=state_deaths.iloc[:,-1]\n",
    "#  Calcualte total new cases and deaths from this week and last week\n",
    "thisweek_cases=state_cases_daily.iloc[:, -7:].sum(axis=1)\n",
    "lastweek_cases=state_cases_daily.iloc[:, -14:-7].sum(axis=1)\n",
    "thisweek_deaths=state_deaths_daily.iloc[:, -7:].sum(axis=1)\n",
    "lastweek_deaths=state_deaths_daily.iloc[:, -14:-7].sum(axis=1)\n",
    "# Combine all of the pulled data into a df\n",
    "combine = pd.DataFrame({'state':state_names,'total_cases':state_totals,'total_deaths':state_total_deaths,'thisweek_cases':thisweek_cases,'lastweek_cases':lastweek_cases,'thisweek_deaths':thisweek_deaths,'lastweek_deaths':lastweek_deaths})\n",
    "# Merge with heatmap to get state name order corrected and add population\n",
    "heatmap=heatmap.merge(combine,on='state',how='right')\n",
    "# Calculate cases per 100k people\n",
    "heatmap['per100k']=(heatmap['total_cases']/(heatmap['population']/100000))\n",
    "# Format state names \n",
    "heatmap['state']=heatmap.state.str.title()\n",
    "\n",
    "# Pull testing data from the covid tracking project\n",
    "url=\"https://covidtracking.com/api/v1/states/current.json\"\n",
    "response = requests.get(url).json()\n",
    "test_data=pd.json_normalize(response)\n",
    "# Calculate the positive testing rate\n",
    "test_data['positive_rate']=(test_data['positive']/test_data['totalTestResults'])*100\n",
    "test_data=test_data[['state','positive_rate']]\n",
    "# Pull state name and match to the testing data based on fip\n",
    "url=\"https://covidtracking.com/api/v1/states/info.json\"\n",
    "response = requests.get(url).json()\n",
    "name_data=pd.json_normalize(response)\n",
    "name_data=name_data[['state','name']]\n",
    "# Merge the DFs into a final DF\n",
    "test_data=test_data.merge(name_data,on='state',how='right')\n",
    "test_data=test_data.drop(columns=['state'])\n",
    "test_data=test_data.rename(columns={'name':'state'})\n",
    "state_heatmap=heatmap.merge(test_data,on='state',how='left')\n",
    "# Formet to prepare for sql\n",
    "state_heatmap.sort_values(by=['state'], inplace=True)\n",
    "state_heatmap=state_heatmap.reset_index()\n",
    "state_heatmap=state_heatmap.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create USA DF by summing the states\n",
    "usa_heatmap=pd.DataFrame(state_heatmap.sum(axis = 0, skipna = True))\n",
    "usa_heatmap=usa_heatmap.T\n",
    "# Drop the rate columns that cant be summed\n",
    "usa_heatmap=usa_heatmap.drop(columns=['state','per100k','positive_rate'])\n",
    "# Convert data type back to int\n",
    "usa_heatmap=usa_heatmap.astype(int)\n",
    "url=\"https://covidtracking.com/api/v1/us/current.json\"\n",
    "#  Pull testing data for the use and calculate the positive rate\n",
    "response = requests.get(url).json()\n",
    "temp=pd.json_normalize(response)\n",
    "usa_heatmap['positive_rate']=(temp['positive']/temp['totalTestResults'])*100\n",
    "usa_heatmap['per100k']=(usa_heatmap['total_cases']/usa_heatmap['population'])*100000\n",
    "# Add in the last data variable from before to save to sql\n",
    "usa_heatmap['lastdate']=last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for county heatmaps\n",
    "# Pull county and state names and current total cases\n",
    "county_fips=county_cases.iloc[:,0]\n",
    "county_names=county_cases.iloc[:,1]\n",
    "state_names=county_cases.iloc[:,2]\n",
    "county_totals=county_cases.iloc[:,-1]\n",
    "# Calcualte total new cases and deaths from this week and last week\n",
    "county_total_deaths=county_deaths.iloc[:,-1]\n",
    "thisweek_cases=county_cases_daily.iloc[:, -7:].sum(axis=1)\n",
    "lastweek_cases=county_cases_daily.iloc[:, -14:-7].sum(axis=1)\n",
    "thisweek_deaths=county_deaths_daily.iloc[:, -7:].sum(axis=1)\n",
    "lastweek_deaths=county_deaths_daily.iloc[:, -14:-7].sum(axis=1)\n",
    "# Combine all of the pulled data into a df\n",
    "county_heatmap=pd.DataFrame({'fips':county_fips,'county':county_names,'state':state_names,'total_cases':county_totals,'total_deaths':county_total_deaths,'thisweek_cases':thisweek_cases,'lastweek_cases':lastweek_cases,'thisweek_deaths':thisweek_deaths,'lastweek_deaths':lastweek_deaths})\n",
    "# Merge with heatmap to get state name order corrected and add population\n",
    "county_heatmap=county_heatmap.merge(county_population,on='fips',how='right')\n",
    "# Calculate cases per 100k people\n",
    "county_heatmap['per100k']=(county_heatmap['total_cases']/(county_heatmap['population']/100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python SQL toolkit and Object Relational Mapper DB1\n",
    "connection1=f'{username1}:{password1}@{host1}:{port1}/{database1}'\n",
    "engine1 = create_engine(f'postgresql://{connection1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to DB1 9733/10000 rows\n",
    "usa_heatmap.to_sql(name='usa_heatmap', con=engine1, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_heatmap.to_sql(name='state_heatmap', con=engine1, if_exists='replace', index=False)\n",
    "county_heatmap.to_sql(name='county_heatmap', con=engine1, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_cases.to_sql(name='county_cases', con=engine1, if_exists='replace', index=False)\n",
    "county_deaths.to_sql(name='county_deaths', con=engine1, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_cases.to_sql(name='state_cases', con=engine1, if_exists='replace', index=False)\n",
    "state_deaths.to_sql(name='state_deaths', con=engine1, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.to_sql(name='orders', con=engine1, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_cases_daily.to_sql(name='state_cases_daily', con=engine1, if_exists='replace', index=False)\n",
    "state_deaths_daily.to_sql(name='state_deaths_daily', con=engine1, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python SQL toolkit and Object Relational Mapper DB2\n",
    "connection2=f'{username2}:{password2}@{host2}:{port2}/{database2}'\n",
    "engine2= create_engine(f'postgresql://{connection2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to DB2 6284/10000 rows\n",
    "county_cases_daily.to_sql(name='county_cases_daily', con=engine2, if_exists='replace', index=False)\n",
    "county_deaths_daily.to_sql(name='county_deaths_daily', con=engine2, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData] *",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
