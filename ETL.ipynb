{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dependencies and Setup\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import time \n",
    "import datetime \n",
    "import numpy as np\n",
    "from config1 import username1,password1,host1,port1,database1\n",
    "from config2 import username2,password2,host2,port2,database2\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, func, inspect, desc\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import requests\n",
    "import json\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Urls for the Johns Hopkins Data\n",
    "confirm_url='https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "death_url='https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "\n",
    "# Read in COVID-19 Files\n",
    "confirm_df=pd.read_csv(confirm_url, error_bad_lines=False)\n",
    "death_df=pd.read_csv(death_url, error_bad_lines=False)\n",
    "# Rename column to match orders data\n",
    "confirm_df=confirm_df.rename(columns={'Province_State': 'State'})\n",
    "death_df=death_df.rename(columns={'Province_State': 'State'})\n",
    "\n",
    "# reindex\n",
    "confirm_df=confirm_df.reset_index()\n",
    "death_df=death_df.reset_index()\n",
    "confirm_df=confirm_df.drop(columns=['index'])\n",
    "death_df=death_df.drop(columns=['index'])\n",
    "\n",
    "# Drop territories\n",
    "death_df.dropna(subset = [\"Admin2\"], inplace=True)\n",
    "confirm_df.dropna(subset = [\"Admin2\"], inplace=True)\n",
    "\n",
    "# Combine State Data\n",
    "state_confirms=confirm_df.groupby('State').sum()\n",
    "state_deaths=death_df.groupby('State').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Clean Confirm data Rows - Remove data for territories and cruise ships, format FIPS\n",
    "confirm_df=confirm_df[confirm_df.Admin2 != 'Unassigned']\n",
    "confirm_df=confirm_df.dropna()\n",
    "confirm_df=confirm_df[~confirm_df['Admin2'].astype(str).str.startswith('Out of')]\n",
    "confirm_df=confirm_df[confirm_df.Admin2 != 'Out of*']\n",
    "confirm_df=confirm_df.reset_index()\n",
    "confirm_df['FIPS']=confirm_df.FIPS.map('{0:0>5.0f}'.format)\n",
    "\n",
    "# Clean Death Data Rows - Remove data for territories and cruise ships\n",
    "death_df=death_df[death_df.Admin2 != 'Unassigned']\n",
    "death_df=death_df.dropna()\n",
    "death_df=death_df[~death_df['Admin2'].astype(str).str.startswith('Out of')]\n",
    "death_df=death_df[death_df.Admin2 != 'Out of*']\n",
    "death_df=death_df.reset_index()\n",
    "death_df['FIPS']=death_df.FIPS.map('{0:0>5.0f}'.format)\n",
    "\n",
    "# Pull County data\n",
    "county_population=death_df[['FIPS','Population']]\n",
    "county_population=county_population.rename(columns={'FIPS':'fips','Population':'population'})\n",
    "county_cases=confirm_df.drop(columns=['UID','index','Country_Region','Lat','Long_','iso2','iso3','code3','Combined_Key'])\n",
    "county_deaths=death_df.drop(columns=['UID','index','Country_Region','Lat','Long_','iso2','iso3','code3','Combined_Key','Population'])\n",
    "county_cases=county_cases.rename(columns={'Admin2':'County'})\n",
    "county_deaths=county_deaths.rename(columns={'Admin2':'County'})\n",
    "\n",
    "last_date=county_cases.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find daily totals - county cases\n",
    "county_cases_daily=county_cases.copy()\n",
    "[r,c]=county_cases_daily.shape\n",
    "for j in range (0,r):\n",
    "    last=0\n",
    "    for i in range (3,c):\n",
    "        current=county_cases_daily.iloc[j,i]-last\n",
    "        last=county_cases_daily.iloc[j,i]\n",
    "        county_cases_daily.iat[j,i]=current\n",
    "        \n",
    "\n",
    "#Find daily totals - county deaths\n",
    "county_deaths_daily=county_deaths.copy()\n",
    "[r,c]=county_deaths_daily.shape\n",
    "for j in range (0,r):\n",
    "    last=0\n",
    "    for i in range (3,c):\n",
    "        current=county_deaths_daily.iloc[j,i]-last\n",
    "        last=county_deaths_daily.iloc[j,i]\n",
    "        county_deaths_daily.iat[j,i]=current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate population data\n",
    "state_deaths=state_deaths.reset_index()\n",
    "state_cases=state_confirms.reset_index()\n",
    "state_population=state_deaths[['State','Population']]\n",
    "# Format state data\n",
    "state_cases=state_cases.drop(columns=['UID','FIPS','code3','Lat','Long_'])\n",
    "state_deaths=state_deaths.drop(columns=['UID','FIPS','code3','Lat','Long_','Population'])\n",
    "\n",
    "\n",
    "#Find daily totals - state cases\n",
    "state_cases_daily=state_cases.copy()\n",
    "[r,c]=state_cases_daily.shape\n",
    "for j in range (0,r):\n",
    "    last=0\n",
    "    for i in range (2,c):\n",
    "        current=state_cases_daily.iloc[j,i]-last\n",
    "        last=state_cases_daily.iloc[j,i]\n",
    "        state_cases_daily.iat[j,i]=current\n",
    "\n",
    "#Find daily totals - state deaths\n",
    "state_deaths_daily=state_deaths.copy()\n",
    "[r,c]=state_deaths_daily.shape\n",
    "for j in range (0,r):\n",
    "    last=0\n",
    "    for i in range (2,c):\n",
    "        current=state_deaths_daily.iloc[j,i]-last\n",
    "        last=state_deaths_daily.iloc[j,i]\n",
    "        state_deaths_daily.iat[j,i]=current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify url\n",
    "url = 'https://www.finra.org/rules-guidance/key-topics/covid-19/shelter-in-place'\n",
    "\n",
    "# Parse HTML Object \n",
    "response = requests.get(url)\n",
    "soup = bs(response.text, 'lxml')\n",
    "\n",
    "# Read Tables\n",
    "tables = pd.read_html(url)\n",
    "table = tables[0]\n",
    "\n",
    "# Remove Excess Columns\n",
    "orders = table[['State', 'Order Date', 'Order Expiration Date']]\n",
    "\n",
    "# Remove Special Charectors from State names\n",
    "orders['State'] = [re.sub(r'[^\\w]', ' ', state) for state in orders['State']]\n",
    "\n",
    "# Initialize lists for date formating\n",
    "od=orders['Order Date']\n",
    "dates=[]\n",
    "\n",
    "# Loop through end dates\n",
    "for date in od:\n",
    "    # Split words and select formatted dates\n",
    "    split = date.split()\n",
    "    res = [i for i in split if '/' in i]\n",
    "    try:\n",
    "        # Select the date and reformate into standard form\n",
    "        res=res[-1]\n",
    "        splitdate = res.split('/')\n",
    "        if len(splitdate[2]) == 2:\n",
    "            splitdate[2]='20'+splitdate[2]\n",
    "        formated = datetime.date(int(splitdate[2]),int(splitdate[0]),int(splitdate[1]))\n",
    "        datestr=str(formated)\n",
    "    except:\n",
    "        # Add null sets for states without expiration dates\n",
    "        datestr=np.nan\n",
    "    # Add formatted dates to list\n",
    "    dates.append(datestr)\n",
    "# Add dates back in\n",
    "orders['Order Date']=dates\n",
    "    \n",
    "# Initialize lists for date formating\n",
    "oed= orders['Order Expiration Date']\n",
    "dates=[]\n",
    "\n",
    "# Loop through end dates\n",
    "for date in oed:\n",
    "    # Split words and select formatted dates\n",
    "    split = date.split()\n",
    "    res = [i for i in split if '/' in i]\n",
    "    try:\n",
    "        # Select the date and reformate into standard form\n",
    "        res=res[-1]\n",
    "        splitdate = res.split('/')\n",
    "        if len(splitdate[2]) == 2:\n",
    "            splitdate[2]='20'+splitdate[2]\n",
    "        formated = datetime.date(int(splitdate[2]),int(splitdate[0]),int(splitdate[1]))\n",
    "        datestr=str(formated)\n",
    "    except:\n",
    "        # Add null sets for states without expiration dates\n",
    "        datestr=np.nan\n",
    "    # Add formatted dates to list\n",
    "    dates.append(datestr)\n",
    "# Add dates back in and correct index\n",
    "orders['Order Expiration Date']=dates\n",
    "\n",
    "orders.iloc[1,2]=str(datetime.date(int(2020),int(4),int(24)))\n",
    "orders.iloc[19,2]=str(datetime.date(int(2020),int(6),int(8)))\n",
    "\n",
    "# Format to merge and Merge\n",
    "orders['State']=[x.strip() for x in orders['State']]\n",
    "orders=orders.merge(state_population,on='State',how='right')\n",
    "# Rename columns to be compatible with sql\n",
    "orders=orders.rename(columns={'State':'state','Order Date': 'order_date','Order Expiration Date':'order_expiration_date','Population':'population'})\n",
    "heatmap=orders.copy()\n",
    "orders=orders.drop(columns=['population'])\n",
    "orders=orders.sort_values(by=['state'])\n",
    "orders=orders.reset_index()\n",
    "orders=orders.drop(columns=['index'])\n",
    "orders.to_csv(r'Orders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for heatmap\n",
    "# Test % Positive\n",
    "state_names=state_cases.iloc[:,0]\n",
    "state_totals=state_cases.iloc[:,-1]\n",
    "state_total_deaths=state_deaths.iloc[:,-1]\n",
    "thisweek_cases=state_cases_daily.iloc[:, -7:].sum(axis=1)\n",
    "lastweek_cases=state_cases_daily.iloc[:, -14:-7].sum(axis=1)\n",
    "thisweek_deaths=state_deaths_daily.iloc[:, -7:].sum(axis=1)\n",
    "lastweek_deaths=state_deaths_daily.iloc[:, -14:-7].sum(axis=1)\n",
    "combine = pd.DataFrame({'state':state_names,'total_cases':state_totals,'total_deaths':state_total_deaths,'thisweek_cases':thisweek_cases,'lastweek_cases':lastweek_cases,'thisweek_deaths':thisweek_deaths,'lastweek_deaths':lastweek_deaths})\n",
    "heatmap=heatmap.merge(combine,on='state',how='right')\n",
    "heatmap['per100k']=(heatmap['total_cases']/(heatmap['population']/100000))\n",
    "heatmap['state']=heatmap.state.str.title()\n",
    "\n",
    "url=\"https://covidtracking.com/api/v1/states/current.json\"\n",
    "response = requests.get(url).json()\n",
    "test_data=pd.json_normalize(response)\n",
    "test_data['positive_rate']=(test_data['positive']/test_data['totalTestResults'])*100\n",
    "test_data=test_data[['state','positive_rate']]\n",
    "url=\"https://covidtracking.com/api/v1/states/info.json\"\n",
    "response = requests.get(url).json()\n",
    "name_data=pd.json_normalize(response)\n",
    "name_data=name_data[['state','name']]\n",
    "test_data=test_data.merge(name_data,on='state',how='right')\n",
    "test_data=test_data.drop(columns=['state'])\n",
    "test_data=test_data.rename(columns={'name':'state'})\n",
    "state_heatmap=heatmap.merge(test_data,on='state',how='left')\n",
    "state_heatmap.sort_values(by=['state'], inplace=True)\n",
    "state_heatmap=state_heatmap.reset_index()\n",
    "state_heatmap=state_heatmap.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_heatmap=pd.DataFrame(state_heatmap.sum(axis = 0, skipna = True))\n",
    "usa_heatmap=usa_heatmap.T\n",
    "usa_heatmap=usa_heatmap.drop(columns=['state','per100k','positive_rate'])\n",
    "usa_heatmap=usa_heatmap.astype(int)\n",
    "url=\"https://covidtracking.com/api/v1/us/current.json\"\n",
    "response = requests.get(url).json()\n",
    "temp=pd.json_normalize(response)\n",
    "usa_heatmap['positive_rate']=(temp['positive']/temp['totalTestResults'])*100\n",
    "usa_heatmap['per100k']=(usa_heatmap['total_cases']/usa_heatmap['population'])*100000\n",
    "usa_heatmap['lastdate']=last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for county heatmaps\n",
    "# Test % Positive\n",
    "# Cases per 100k\n",
    "county_fips=county_cases.iloc[:,0]\n",
    "county_names=county_cases.iloc[:,1]\n",
    "state_names=county_cases.iloc[:,2]\n",
    "county_totals=county_cases.iloc[:,-1]\n",
    "county_total_deaths=county_deaths.iloc[:,-1]\n",
    "thisweek_cases=county_cases_daily.iloc[:, -7:].sum(axis=1)\n",
    "lastweek_cases=county_cases_daily.iloc[:, -14:-7].sum(axis=1)\n",
    "thisweek_deaths=county_deaths_daily.iloc[:, -7:].sum(axis=1)\n",
    "lastweek_deaths=county_deaths_daily.iloc[:, -14:-7].sum(axis=1)\n",
    "county_heatmap=pd.DataFrame({'fips':county_fips,'county':county_names,'state':state_names,'total_cases':county_totals,'total_deaths':county_total_deaths,'thisweek_cases':thisweek_cases,'lastweek_cases':lastweek_cases,'thisweek_deaths':thisweek_deaths,'lastweek_deaths':lastweek_deaths})\n",
    "county_heatmap=county_heatmap.merge(county_population,on='fips',how='right')\n",
    "county_heatmap['per100k']=(county_heatmap['total_cases']/(county_heatmap['population']/100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Arrays\n",
    "[county_cases1,county_cases2,county_cases3]=np.array_split(county_cases, 3)\n",
    "[county_deaths1,county_deaths2,county_deaths3]=np.array_split(county_deaths,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python SQL toolkit and Object Relational Mapper DB1\n",
    "connection1=f'{username1}:{password1}@{host1}:{port1}/{database1}'\n",
    "engine1 = create_engine(f'postgresql://{connection1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to DB1 9817/10000 rows\n",
    "usa_heatmap.to_sql(name='usa_heatmap', con=engine1, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_heatmap.to_sql(name='state_heatmap', con=engine1, if_exists='replace', index=False)\n",
    "county_heatmap.to_sql(name='county_heatmap', con=engine1, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_cases1.to_sql(name='county_cases1', con=engine1, if_exists='replace', index=False)\n",
    "county_cases2.to_sql(name='county_cases2', con=engine1, if_exists='replace', index=False)\n",
    "county_cases3.to_sql(name='county_cases3', con=engine1, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_deaths1.to_sql(name='county_deaths1', con=engine1, if_exists='replace', index=False)\n",
    "county_deaths2.to_sql(name='county_deaths2', con=engine1, if_exists='replace', index=False)\n",
    "county_deaths3.to_sql(name='county_deaths3', con=engine1, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_cases.to_sql(name='state_cases', con=engine1, if_exists='replace', index=False)\n",
    "state_deaths.to_sql(name='state_deaths', con=engine1, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.to_sql(name='orders', con=engine1, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_cases_daily.to_sql(name='state_cases_daily', con=engine1, if_exists='replace', index=False)\n",
    "# state_deaths_daily.to_sql(name='state_deaths_daily', con=engine1, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python SQL toolkit and Object Relational Mapper DB1\n",
    "# connection2=f'{username2}:{password2}@{host2}:{port2}/{database2}'\n",
    "# engine2= create_engine(f'postgresql://{connection2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Write to DB2 6284/10000 rows\n",
    "# county_cases_daily.to_sql(name='county_cases_daily', con=engine2, if_exists='replace', index=False)\n",
    "# county_deaths_daily.to_sql(name='county_deaths_daily', con=engine2, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData] *",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
